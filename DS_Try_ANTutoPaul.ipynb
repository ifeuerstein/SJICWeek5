{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS Try ANTutoPaul.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifeuerstein/SJICWeek5/blob/main/DS_Try_ANTutoPaul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAT7xLYe_scU"
      },
      "source": [
        "# An example scraper using a list\n",
        "\n",
        "The code below can be copied and adapted to create your own scraper.\n",
        "\n",
        "The first part installs all the libraries. I've kept this separate to the other parts so that you don't have to install them every time you want to run the scraper itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE2vW-IX9kYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef9bf1a-26c6-4884-8232-d89c01429f41"
      },
      "source": [
        "#install the libraries \n",
        "#scraperwiki is a library for scraping webpages\n",
        "!pip install scraperwiki\n",
        "import scraperwiki\n",
        "#lxml.html is used to convert it into xml (more structured)\n",
        "import lxml.html\n",
        "#cssselect is used to drill down into that and find data in tags\n",
        "!pip install cssselect\n",
        "import cssselect\n",
        "#the pandas library which is used to work with data - we call it 'pd' here so we have to type less!\n",
        "import pandas as pd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scraperwiki in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (2.23.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from scraperwiki) (1.3.23)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (1.1.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (2.8.1)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->scraperwiki) (1.0.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->scraperwiki) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->scraperwiki) (1.1.1)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MTFOTQUh2UH"
      },
      "source": [
        "## Using a list\n",
        "\n",
        "Below we write some code to create a list of counties that can be used to generate URLs on a karting site.\n",
        "\n",
        "We also store the 'base URL' that we will add to each item in the list to create a full URL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5eL7rQDp-s"
      },
      "source": [
        "#create a list of counties that we will need to generate URLs\r\n",
        "pages = [\"1\",\"2\",\"3\",\"4\"]\r\n",
        "#store the base URL we will add those to\r\n",
        "baseurl = \"https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn8Zsx4fiC_u"
      },
      "source": [
        "## Using a loop\n",
        "\n",
        "Next we loop through each item in the list and add it to that base url using the `+` operator.\n",
        "\n",
        "We add a `print` function inside the loop to check that it works each time - and copy those links into a browser to check that they are the right links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO6lht-cgc4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de713a0-90d5-4d42-deb8-62e43bdc52bc"
      },
      "source": [
        "#start looping through our list\n",
        "for i in pages:\n",
        "  fullurl = baseurl+i\n",
        "  print(fullurl)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=1\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=2\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=3\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5s0_uuJiNIl"
      },
      "source": [
        "## Scraping each URL as we loop\n",
        "\n",
        "Now that we know the loop works in generating the right URLs, we can extend the code inside the loop so that it *scrapes* each URL.\n",
        "\n",
        "At this point we are using some of the libraries we imported at the start. `scraperwiki.scrape()`, for example, is the `scrape()` function from the `scraperwiki` library. \n",
        "\n",
        "Let's look at the code first, and then explain it..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yw0QKTJ_qpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcce2a0b-86a7-4372-cf4f-604c53425b45"
      },
      "source": [
        "#start looping through our list\n",
        "for i in pages:\n",
        "  fullurl = baseurl+i\n",
        "  print(fullurl)\n",
        "  #Scrape the html at that url\n",
        "  html = scraperwiki.scrape(fullurl)\n",
        "  # turn our HTML into an lxml object\n",
        "  root = lxml.html.fromstring(html) \n",
        "  #We scrap the rows of the table\n",
        "  rows = root.cssselect('tr')\n",
        "  #The first row is a heading row with only one <td> so we start from the second row\n",
        "  for i in rows[1:8]:\n",
        "    #grab all the td tags\n",
        "    tds = i.cssselect('td')\n",
        "    #grab the second tag because it contains the code\n",
        "    num = tds[0].text_content()\n",
        "    #grab 3rd\n",
        "    emplacement = tds[2].text_content()\n",
        "    #grab 4th\n",
        "    tdspans = i.cssselect('td span')\n",
        "    tdps = i.cssselect('td p')\n",
        "    party = tdspans[0].text_content()\n",
        "    politician = tdps[1].text_content()\n",
        "    #grab 5th\n",
        "    etat = tds[4].text_content()\n",
        "    #grab 6th\n",
        "    sort = tds[5].text_content()\n",
        "    #grab the 7th because it contains the date d'examen\n",
        "    datedexamen = tds[6].text_content()\n",
        "    #grab the 8th\n",
        "    datedepot = tds[6].text_content()\n",
        "  #print\n",
        "  print(politician)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=1\n",
            "Mme ValÃ©rie Rabault\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=2\n",
            "M. Julien Aubert\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=3\n",
            "M. Pierre Cordier\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=4\n",
            "M. Sacha HouliÃ©\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDFgv8F5kt6U"
      },
      "source": [
        "## The functions we are using\n",
        "\n",
        "Let's break some of this down.\n",
        "\n",
        "So `scraperwiki.scrape()` is the `scrape()` function from the `scraperwiki` library. The *ingredient* we give to that function is the URL we stored in the `fullurl` variable.\n",
        "\n",
        "The `scrape()` function basically fetches the whole webpage at a given address (the ingredient it's given).\n",
        "\n",
        "The results of running that function are stored in a new variable called `html`.\n",
        "\n",
        "This isn't in a form we can easily work with, yet, so we need another function to convert it to something we can drill down into. \n",
        "\n",
        "That function is the `fromstring()` function from the `lxml.html` library. The *ingredient* we give to that function is the `html` variable we just created.\n",
        "\n",
        "The results are stored in another new variable, `root`.\n",
        "\n",
        "This variable is a particular type of object (an \"lxml object\" if you need to know) that can be drilled down into using the `cssselect` function. That function will grab elements that match the *CSS selectors* that you give it as an ingredient.\n",
        "\n",
        "In this case we specify `'h2'`, which means \"any h2 tag\" - so it will grab the contents of any h2 tags in the page.\n",
        "\n",
        "Don't worry about memorising any of the code above: this is code that you can re-use time and time again. The only bit you will need to change is the selector, in order to specify the particular HTML you're after. \n",
        "\n",
        "To work out the selector you need, you'll often need to Google around, learning as you go, but selectors are pretty easy to get the hang of, and I'll talk about it more below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1n21e_LWI-A"
      },
      "source": [
        "## Using CSS selectors\n",
        "\n",
        "**CSS selectors** are used to target different elements in a HTML page. A basic selector can target just one type of HTML tag, like `<h2>` or `<p>`, but you can also target a combination of tags (such as any `<strong>` tags within `<p>` tags). \n",
        "\n",
        "More complicated selectors can also be used to target tags based on their attributes (e.g. not just `<p>` but specifically `<p class=\"summary\">`).\n",
        "\n",
        "You can find lots of resources to help you with CSS selectors, such as [this one](https://www.w3schools.com/cssref/css_selectors.asp). Many will relate to styling webpages (which is how CSS selectors are most often used - selectors are used to target the HTML elements that you want to style), but the principles are the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQpygiZfYeT4"
      },
      "source": [
        "## Saving the information we've grabbed.\n",
        "\n",
        "Now we've grabbed some information we can extend the code further to save it.\n",
        "\n",
        "At this point we need to use functions from another library: `pandas`. This is a library for data storage and analysis. When we imported `pandas` we called it `pd` for short. This is quite common. Any reference to `pd` in the code, then, means `pandas`\n",
        "\n",
        "First, we use the function `DataFrame()` which creates a pandas dataframe. As ingredients it needs to know the names of any columns.\n",
        "\n",
        "You will see below that we add a line *before* the loop which uses that to create an empty dataframe to store the data in.\n",
        "\n",
        "Then, inside the loop, the data we extract is added to the dataframe.\n",
        "\n",
        "Here's the code first - then I'll explain the new bits after.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdUvnMlj6bHA",
        "outputId": "2f08d438-9ad0-4229-d2e1-ab4577174ec4"
      },
      "source": [
        "#Create a dataframe to store the data we are about to scrape\r\n",
        "#It has one column called 'title'\r\n",
        "#We call this dataframe 'df'\r\n",
        "df = pd.DataFrame(columns=[\"num\",\"emplacement\",\"party\",\"politician\",\"etat\",\"sort\",\"datedexamen\",\"datedepot\"])\r\n",
        "\r\n",
        "#start looping through our list\r\n",
        "for i in pages:\r\n",
        "  fullurl = baseurl+i\r\n",
        "  print(fullurl)\r\n",
        "  #Scrape the html at that url\r\n",
        "  html = scraperwiki.scrape(fullurl)\r\n",
        "  # turn our HTML into an lxml object\r\n",
        "  root = lxml.html.fromstring(html) \r\n",
        "  #We scrap the rows of the table\r\n",
        "  rows = root.cssselect('tr')\r\n",
        "  #The first row is a heading row with only one <td> so we start from the second row\r\n",
        "  for i in rows[1:8]:\r\n",
        "    #grab all the td tags\r\n",
        "    tds = i.cssselect('td')\r\n",
        "    #grab the second tag because it contains the code\r\n",
        "    num = tds[0].text_content()\r\n",
        "    #grab 3rd\r\n",
        "    emplacement = tds[2].text_content()\r\n",
        "    #grab 4th\r\n",
        "    tdspans = i.cssselect('td span')\r\n",
        "    tdps = i.cssselect('td p')\r\n",
        "    party = tdspans[0].text_content()\r\n",
        "    politician = tdps[1].text_content()\r\n",
        "    #grab 5th\r\n",
        "    etat = tds[4].text_content()\r\n",
        "    #grab 6th\r\n",
        "    sort = tds[5].text_content()\r\n",
        "    #grab the 7th because it contains the date d'examen\r\n",
        "    datedexamen = tds[6].text_content()\r\n",
        "    #grab the 8th\r\n",
        "    datedepot = tds[6].text_content()\r\n",
        "    #now store both in a dataframe\r\n",
        "    df = df.append({\r\n",
        "      \"num\" : num,\r\n",
        "      \"datedexamen\" : datedexamen, \"emplacement\": emplacement, \"parti\":party,\"député\": politician, \"etat\":etat , \"sort\":sort , \"datedexamen\":datedexamen , \"datedepot\":datedepot\r\n",
        "      }, ignore_index=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=1\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=2\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=3\n",
            "https://www.assemblee-nationale.fr/dyn/15/amendements?dossier_legislatif=DLR5L15N40245&examen=EXANR5L15PO59048B3360P1D1&page=4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omuqBRQKZW8h"
      },
      "source": [
        "## The new code\n",
        "\n",
        "The first line of new code is this:\n",
        "\n",
        "`df = pd.DataFrame(columns=[\"title\"])`\n",
        "\n",
        "We are creating a new variable here, called `df`, and assigning to it the results of using a function: `pd.DataFrame()` (the `pandas` function `DataFrame`).\n",
        "\n",
        "That takes an ingredient which specifies the columns as being a list (note the square brackets) of one string: `\"title\"`.\n",
        "\n",
        "The second line of new code is this:\n",
        "\n",
        "```\n",
        "df = df.append({\n",
        "      \"title\" : title\n",
        "      }, ignore_index=True)\n",
        "```\n",
        "\n",
        "This takes the `df` variable and updates it. \n",
        "\n",
        "On the right of the equals sign is `df.append()` - this means it is using a function called `append` to append (add) new data to the `df` variable it's attached to.\n",
        "\n",
        "The `append` function [can include various ingredients](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html): firstly the data that you want to append to the dataframe; but also settings, such as whether you want something called `ignore_index` to be `True` or `False`. Setting this to `True` just avoids problems when your data isn't unique.\n",
        "\n",
        "What about the data that you are appending? Well, this has to be in the form of a **dictionary**. A dictionary is like a list, but with two key differences: firstly that it uses curly brackets instead of square ones: `{}`, and secondly it's a list of *pairs*: a 'key', and a 'value', separated by a colon.\n",
        "\n",
        "Here's the dictionary in our code:\n",
        "\n",
        "`{\"title\" : title}`\n",
        "\n",
        "The first part, `\"title\"` is the **key**. This matches the column heading in the empty data frame. Note that it's a **string**: a label, basically.\n",
        "\n",
        "The second part, `title`, is the **value**. This isn't in quotes so it's not a string - it's a variable. A few lines earlier we created this variable with `title = i.text_content()`\n",
        "\n",
        "So having extracted that information and stored it in `title`, the line of code is storing it in a dataframe with the label (key) \"title\":\n",
        "\n",
        "```\n",
        "df = df.append({\n",
        "      \"title\" : title\n",
        "      }, ignore_index=True)\n",
        "```\n",
        "\n",
        "We can print the dataframe to see what's in there:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIdecPU__881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddf4201-c088-45e1-a7c7-32c22f3f0c45"
      },
      "source": [
        "#Once the loop has finished we can take a look at the data\n",
        "print(df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  num  ...                                              parti\n",
            "0   \\n                                            ...  ...                           LibertÃ©s et Territoires\n",
            "1   \\n                                            ...  ...                         Socialistes et apparentÃ©s\n",
            "2   \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "3   \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "4   \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "5   \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "6   \\n                                            ...  ...                         Socialistes et apparentÃ©s\n",
            "7   \\n                                            ...  ...  Mouvement DÃ©mocrate (MoDem) et DÃ©mocrates ap...\n",
            "8   \\n                                            ...  ...  Mouvement DÃ©mocrate (MoDem) et DÃ©mocrates ap...\n",
            "9   \\n                                            ...  ...                                      Agir ensemble\n",
            "10  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "11  \\n                                            ...  ...                                      Agir ensemble\n",
            "12  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "13  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "14  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "15  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "16  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "17  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "18  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "19  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "20  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "21  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "22  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "23  \\n                                            ...  ...                           La RÃ©publique en Marche\n",
            "24  \\n                                            ...  ...                           La RÃ©publique en Marche\n",
            "25  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "26  \\n                                            ...  ...                                  Les RÃ©publicains\n",
            "27  \\n                                            ...  ...                           La RÃ©publique en Marche\n",
            "\n",
            "[28 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh73Sbovc9To"
      },
      "source": [
        "## Exporting the data\n",
        "\n",
        "The `pandas` library has another function for exporting data: `to_csv()`.\n",
        "\n",
        "It needs to be attached to the name of the dataframe variable with a period, then, in the brackets, you specify the name of the file you want to export it as. Make sure this ends in '.csv' so it can be used in a spreadsheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9FDGDX0__eA"
      },
      "source": [
        "#And we can export it\n",
        "df.to_csv(\"TableauANFINAL.csv\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCtq8ih5dNaE"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Once exported, it should appear in the file explorer in Google Colab on the left hand side. Click on the folder icon to open this up and you should see the file you just created (there's a refresh button above if you can't).\n",
        "\n",
        "Hover over the file name to see three dots, then click on those to select **Download** and download to your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq60rrHrAoeN"
      },
      "source": [
        "## How to adapt it\n",
        "\n",
        "You can use most of this code without having to change it. All you *need* to change is the lines specifying the base URL, and the list of words to add to it.\n",
        "\n",
        "And this line, which specifies what you want to scrape from that page:\n",
        "\n",
        "`titles = root.cssselect('h2')`\n",
        "\n",
        "If you're scraping one type of information from one page, that will be enough. \n",
        "\n",
        "For the CSS selector you will need to identify the HTML in the page you are scraping, and the combination of tags that is being used. \n",
        "\n",
        "Some [reading around CSS selectors](https://www.w3schools.com/cssref/css_selectors.asp) will help you here, but a couple of useful things to know include:\n",
        "\n",
        "* A period `.` means `class=\"`\n",
        "* A hash `#` means `id=\"`\n",
        "\n",
        "So `'div.title a'` means `<div class=\"title\"><a ...>` - or, in other words, anything on the page inside an `<a>` tag (a link) within a `<div class=\"title\">` tag.\n",
        "\n",
        "The words used for variables (like \"baseurl\" and \"titles\" above) may not be relevant to what you are scraping - but that doesn't matter, because those words are arbitrary. If you do decide to change them, make sure you change them *throughout* the code, or it will create an error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE35f1sxgV6Q"
      },
      "source": [
        "## Generating URLs for a scraper to loop through\n",
        "\n",
        "Alternatively you might *generate* the URLs: for example, if they end in a number that goes up by 1 each time you can use `range` to generate that list of numbers and add them to the URL using `+`.\n",
        "\n",
        "However, you cannot mix numbers and strings, so you need to convert the numbers to a string as you do this. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUcpB_cPNwq5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "b7183717-1963-44bd-b23c-108433c48c7b"
      },
      "source": [
        "#Create the basic URL that appears before the number\n",
        "baseurl = \"http://mypage.com?page=\"\n",
        "#Create a list of numbers to put on the end\n",
        "pagenums = range(1,11)\n",
        "#Now generate the URLs by looping through the list and adding it to the URL\n",
        "for i in pagenums:\n",
        "  #Combine the two - \n",
        "  #this will generate an error because we are trying to combine a string and a number\n",
        "  fullurl = baseurl+i"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fe2fd370c033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m#Combine the two -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#this will generate an error because we are trying to combine a string and a number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mfullurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseurl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: must be str, not int"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshYh7u7ON5d"
      },
      "source": [
        "## Tip: converting numbers into strings\n",
        "\n",
        "You can see the error `must be str, not int` - in other words the second part must be a string not an integer.\n",
        "\n",
        "To fix that you can use the `str()` function, which will convert a number into a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyuZ131lOW3w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "475d3e46-8694-42eb-f546-23e7cc7882d5"
      },
      "source": [
        "#Create the basic URL that appears before the number\n",
        "baseurl = \"http://mypage.com?page=\"\n",
        "#Create a list of numbers to put on the end\n",
        "pagenums = range(1,11)\n",
        "#Now generate the URLs by looping through the list and adding it to the URL\n",
        "for i in pagenums:\n",
        "  #Convert i to a string\n",
        "  i = str(i)\n",
        "  #Combine the two\n",
        "  fullurl = baseurl+i\n",
        "  #print it\n",
        "  print(fullurl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://mypage.com?page=1\n",
            "http://mypage.com?page=2\n",
            "http://mypage.com?page=3\n",
            "http://mypage.com?page=4\n",
            "http://mypage.com?page=5\n",
            "http://mypage.com?page=6\n",
            "http://mypage.com?page=7\n",
            "http://mypage.com?page=8\n",
            "http://mypage.com?page=9\n",
            "http://mypage.com?page=10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOvob6z3hEmq"
      },
      "source": [
        ""
      ]
    }
  ]
}